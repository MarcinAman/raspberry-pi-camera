\documentclass{article}
\usepackage{polski}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage[utf8]{inputenc}

\title{Przetwarzanie obrazu}
\author{Marcin Aman, Bartek Tonia}
\date{Grudzień 2018}


\begin{document}

\maketitle

\section{Wstęp}
Głównym projekt bylo pokazanie możliwości Raspberry Pi 3 w zakresie przetwarzania obrazu.

Projekt został zrealizowany przy użyciu biblioteki OpenCV, języka Python oraz modułu umożliwiającego przechwytywanie obrazu na wyżej wymienionym urządzeniu.

\section{Wymagania wstępne}
Aby uruchomić program należy:
\begin{enumerate}[I]
    \item posiadać zainstalowane środowisko Python 3
    \item posiadać skompilowaną bibliotekę zgodnie z instrukcją od developerów lub zainstalowane komendą "pip install opencv-python" wraz z biblioteką numpy
    \item posiadać moduł umożliwiający przechwytywanie obrazu, na przykład kamerę internetową na USB
\end{enumerate}
Opcjonalnie, aby uruchomić serwer jest potrzebne dodatkowe urządzenie zainstalowanym językiem Python 3 oraz OpenCV i posiadanie wspólnej podsieci dla tych komputerów.
\\
Oprogramowanie można uruchomić poprzez polecenie "python main.py [tryb]", gdzie tryb to nazwa trybu przetwarzania obrazu. Jeśli dany tryb nie istnieje zostanie przedstawiony odpowieni wyjątek.

\subsection{OpenCV}
Bibiloteka OpenCV służy do przechwytywania oraz przetwarzania obrazu. Jest dostępna na przy użyciu różnych języków programowania jak Python, Java czy C++. Dodatkowe jej funkcjonalności polegają na wsparciu dla uczenia maszynowego czy fotografii komputerowej.\\

W naszym projekcie skupiliśmy się głównie na przetwarzaniu obrazu. Obraz jest reprezentowany jako 3 wymiarowa macierz, gdzie na osi X oraz Y są współrzędne a Z to wartości czerwonego, zielonego oraz niebieskiego w danym pikselu czyli paleta RGB.

\section{Funkcjonalności}
\subsection{Detekcja twarzy}
Pierwszą wykonaną funkcjonalnością była detekcja twarzy. Biblioteka udostępnia funkcje zaręwno do stwierdzenia w ktorym obszarze jest twarz jak i do do prostego przetworzenia klatki. W przypadku ninejeszego projektu jest to zakreślenie prostokąta na danym obszarze:

\begin{center}
    \includegraphics[scale=0.25]{face-detection.png}
\end{center}

Podstawy teoretyczne działania: \\
Kazda klatka jest procesowana przez algorytm wykrywania twarzy, ktory bazuje na pewnym zbiorze danych. Podczas tego procesu jest wykrywany zbiór cech charakterystycznych a następnie są one porównywane z modelem. Jest to podejscie oparte na koncepcji uczenia maszynowego, a sam model jest dostępny na stronie OpenCV, przez co użytkownik nie jest zmuszony do jego trenowania.

Sam algorytm nosi nazwę "haar cascades". Dziala on na podstawie wyekstrachowania z kazdej klatki grup wartosci specyficznych oraz porowania ich z tym co znajduje sie w modelu. Bierze on pod uwagę 3 typy takich wartości:

\begin{center}
    \includegraphics[scale=0.7]{haar_features.jpg}
\end{center}

Każda wartość jest wyliczana jako różnica sum wartości w części ciemnej i jasnej.

W przypadku użycia prostego algorytmu iteracyjnego porównania każdego fragmentu przetwarzanej klatki z modelem byłaby to operacja praktycznie nie możliwa do wykonania na urządzeniach o ograniczonej mocy obliczeniowej jak Raspberry Pi. Nawet dla zdjęcia o rozmiarach 24x24 pikseli oznaczałoby to około 160000 punktów charakterystycznych. Dlatego tworcy OpenCV zaimplementowali pewną aproksymację polegajacą na zalożeniu, ze wiekszość porównań o danej specyfice nie będzie nas przybliżać do rozwiązania. Przykładem tego jest poniższa grafika:

\begin{center}
    \includegraphics[scale=0.7]{aproximation.png}
\end{center}

Grafiki w górnym rzędzie są pomocne w wykrywaniu twarzy. Pierwsza z nich ma na celu wykrycie regionu oczu (są one zwykle ciemniejsze nią koniec nosa i policzki) a drugi obrazek jest przeznaczony do samego nosa i oczu. W tym przypadku rząd drugi nie pomoże nam w stwierdzeniu czy i gdzie jest twarz, gdyż regiony w których szukamy nie są na odpowiedniej wysokości. Taki proces jest realizowany po części za pomocą algorytmu Adaboost, który pozwala podczas kolejnych iteracji stwierdzić które regiony są słabymi klasyfikatorami a które silnymi i sprawiając, że zwracana jest na nie mniejsza uwaga.\\
% https://pl.wikipedia.org/wiki/AdaBoost

Jak to wyglada na Raspberry Pi? \\
Pomimo optymalizacji otrzymujemy okolo 10 klatek na sekunde przy obrazie przetwarzanym na żywo. Jest to wynik ilości obliczeń jakie są potrzebne na przetworzenie jednej klatki.
\\
W celu uruchomienia funkcjonalności należy posłużyć się komendą "python main.py face-detection".

\subsection{Detekcja twarzy i oczu}
Poprzednio przedstawiony algorytm detekcji twarzy można jeszcze roszerzyć o detekcję oczu. W tym przypadku jest to jednak trudniejsze zadanie, gdyż oczy będą rozpoznawane przez znalezienie ciemnego obszaru (oka), jaśniejszego (nosa) oraz znowu ciemniejszego, czyli kolejnego oka. Bardzo podobną charakterystykę posiada nos człowieka z racji tego będziemy obserwować dość dużą ilość błędnych detekcji. Dodatkowym utrudnieniem są tutaj okulary, które również zaburzają wzór.\\

\begin{center}
    \includegraphics[scale=0.25]{eyes_face_detection.png}
\end{center}

W przypadku tego zadania urządzenie Raspberry Pi było wstanie przetworzyć znacznie mniejszą ilość klatek niż przy detekcji samej twarzy, gdyż ilość obliczeń na klatkę znacznie wzrosła.
\\
W celu uruchomienia funkcjonalności należy posłużyć się komendą "python main.py eyes-face-detection".

\subsection{Detekcja krawędzi}
Celem uzyskania detekcji krawędzi posłużono się operatorem Sobel-a.
Operator Sobel-a to operator różniczkowania pozwalający na przybliżenie pochodnych kierunkowych w 8 kierunkach (czyli co 45 stopni).
Podczas przetwarzania klatki badamy zmiany intensywności pikseli za pomoca pochodnych. Z racji tego, iż obraz jest dwuwymiarowy a funkcja intensywności ma rozklad dyskretny. Dalej uzyskujemy przybliżenie gradientu funkcji intensywności. Poszukujemy następnie maksimow lokalnych uznajac je za krawedzie. Przykład dla jednowymiarowej funkcji jest następujący:
\begin{center}
    \includegraphics[scale=1]{sobel1.jpg}
\end{center}
\begin{center}
    \includegraphics[scale=1]{sobel2.jpg}
\end{center}

W celu obliczenia takiego operatora należy najpierw obliczyć sploty po X i Y a następnie złączyć je razem: \\
\begin{center}
    \begin{align*}
 G_x = \left[
        \begin{array}{ccc}
         -1 & 0 & 1\\
         -2 & 0 & 2\\
         -1 & 0 & 1
         \end{array}
      \right]
      * I
      \\
      G_y = \left[
        \begin{array}{ccc}
          1 & 2 & 1\\
          0 & 0 & 0\\
         -1 & -2 & -1
         \end{array}
      \right]
      * I
      \\
      G= \sqrt{G_x^2 + G_y^2}
\end{align*}
\end{center}
\\
Na początku należy zdecydowac jaka bedzie macierz która będzie przetwarzana. Zwykle jest to 3x3 i tak też zostało zaimplementowane, gdyź wyższe stopnie macierzy będą znacznie trudniejsze do przetworzenia. W przypadku tego operatora rezulatem dla kazdego punktu bedzie albo wartosc wektora, albo jego norma.\\

Przetwarzając obraz wyszególnionym operatorem na Raspberry Pi otrzymano zadowalające wyniki. Płynność obrazu była na wysokim poziomie ze względu na fakt, iż sama operacja nie wymaga wielu obliczeń.

\begin{center}
    \includegraphics[scale=0.25]{sobel.png}
\end{center}

\\
W celu uruchomienia funkcjonalności należy posłużyć się komendą "python main.py sobel".

\subsection{Filtrowanie tła}
Kluczową funkcjonalnością względem praktycznego zastosowania jest filtrowanie tła. Problem jest bardziej złożony niż wydaje się na początku, gdyż w najprostszym przypadku redukuje się do posiadanie obrazu tła oraz obrazu z frontem i ekstakcji elementów znajdujących się na froncie. Problem pojawia się w momencie, gdy zarówno tło jak i kamera się porusza. W tym przypadku nie dysponujemy obrazem tła.

\begin{center}
    \includegraphics[scale=0.25]{backgroudn.png}
\end{center}

Metoda polega na algorytmach Gaussa do ekstrakcji tła/frontu. Ze wzgledu na zlożoność problemu jest używana pewna heurystyka do stwierdzenia ktore elementy sa tłem a ktore frontem. Dlatego też metoda nie jest idealna. Na przykładzie widac szum w tle. Dodatkowo problem zaczyna się w momencie gdy obiekt sie porusza, czasami zdarza sie, ze jest on w wiekszosci biały, co sugeruje iż został uznany w całości jako front. Algorytm bardzo dobrze sobie radzi z obiektami oddalonymi o wieksza odleglość i odeseparowaniem ich od tych bliskich. Idealnymi warunkami dla algorytmu są sytuacje, gdzie mała część obrazu jest ruchoma a reszta statyczna:

\begin{center}
    \includegraphics[scale=0.6]{resmog2.jpg}
\end{center}

\\
W celu uruchomienia funkcjonalności należy posłużyć się komendą "python main.py background-filter".

\subsection{Odszumianie obrazu}
Z popularnych metod jak Gaussian Bluring, Median Bluring wybrano jedną: "Non-local means denosing" ze względu na jej obiecujące rezulaty. Polega ona na bardzo prostym zalożeniu, że jesli wystepuje anomalia w danym obszarze to najprawdopodobniej jest ona szumem. Non-local means denosing w przeciwieństwie do metod lokalnych uwzględnia normę ze wszystkich pikseli obrazu z wagami będącymi stopniem podobieństwa do aktualnie przetwarzanego piksela. \\
\\
$$u(p) = 1/C(p) \int_{\Omega} v(q)f(p,q)dq $$
\\
Normalizacja:
\\
$$C(p) = \int_{\Omega} f(p,q)dq $$ \\
\\
p,q - punkty obrazu\\
u(p) - wynik filtra w punktcie p\\
v(q) - nieprzefiltrowana wartość punktu q\\
f(p,q) - waga\\
\\
Dla przestrzeni dyskretnej takiej jak zbiór pikseli operacja redukuje się do calki po p.

\begin{center}
    \includegraphics[scale=0.3]{smooting.png}
\end{center}

Sama metoda jest bardzo wymagająca pod względem obliczeniowym. Z racji tego mały komputer jak Raspberry Pi 3 nie jest wstanie zapewnić płynnego przetwarzania klatek w czasie rzeczywistym. W przeprowadzonych testach otrzymano średnio około 0.5 klatki na sekundę.

\\
W celu uruchomienia funkcjonalności należy posłużyć się komendą "python main.py smoothing".

\subsection{Wykrywanie krawędzi przy użyciu operatora Laplac-a}
Operator Sobela zakłada kalkulację pierwszych pochodnych z funkcji intensywności w celu znalezienia maksimow lokalnych. Operator Laplaca zaś zakłada policzenie drugich pochodnych celem znalezienia miejsc zerowych. One właśnie będą sugerowały występowanie w krawędzi w tym punkcie. Koncepcja polega w znacznym stopniu na intuicyjnym rozumieniu drugiej pochodnej fukcji, jest to szybkość zmiany wartości.\\
\begin{center}
    \includegraphics[scale=1]{laplace.jpg}
\end{center}

$$ L(f) = \frac{\partial^2f}{\partial x^2} + \frac{\partial^2f}{\partial y^2} $$\\

\begin{center}
    \includegraphics[scale=0.3]{laplace.png}
\end{center}

Przechwycony obraz posiada znaczną ilość zakłóceń które objawiają się poprzez "śnieg" na grafice. W tym przypadku są to fałszywe miejsca zerowe drugiej pochodnej.


\\
W celu uruchomienia funkcjonalności należy posłużyć się komendą "python main.py laplacian".

\section{Zdalne przetwarzanie obrazu}
W celu skorzystania z możliwości zdalnego przetwarzania obrazu należy posiadać komputer z zainstalowaną biblioteką OpenCV, numpy oraz językiem Python. Dodatkowo, konieczne jest zapewnienie połączenia urządzenia z komputerem-serwerem. Polecane jest tutaj podłączenie do jednej podsieci, gdyż nie będzie wymagane wtedy skonfigurowanie routingu. \\

W celu uruchomienia programu po stronie klienta (Raspberry Pi) należy wywołać komendę "python client.py [ip host-a] [ip kilenta]". Po tym w konsoli powinien pojawić się komunikat: "Done?". Kolejno należy podłączyć serwer przez "python server [ip hosta] [ip klienta[ [tryb]". Jeśli połączenie zostało nawiązane to w terminalu klienta pojawi się: "Connected by: [ip hosta]".Po tym można już rozpocząć przetwarzanie wprowadzająć dowolny znak po stronie klienta i klikając enter. \\
Sam mechanizm działa na podstawie socketów sieciowych. Raspberry Pi przechwytuje klatkę, następnie jest ona przesyłana do serwera, który zajmuje się jej przetworzeniem. Kolejnym krokiem jest odesłanie klatki do Raspberry i wyświetlenie jej po stronie klienta. \\
Mechanizm pozwala na bardzo wydajne przetwarzanie obrazu, jednakże wprowadza opóźnienie związane z prędkością przesyłu danych przez sieć oraz ich przetwarzania po stronie serwera.
\end{document}
